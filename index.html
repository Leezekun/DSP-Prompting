<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Directional Stimulus Prompting">
  <meta property="og:title" content="Guiding Large Language Models via Directional Stimulus Prompting"/>
  <meta property="og:description" content="Directional Stimulus Prompting"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Directional Stimulus Prompting">
  <meta name="twitter:description" content="Guiding Large Language Models via Directional Stimulus Prompting">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="guiding, large language models, directional stimulus prompting, prompt optimization, prompting engineering, hint prompting">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Guiding Large Language Models via
    Directional Stimulus Prompting
    </title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Guiding Large Language Models via
              Directional Stimulus Prompting
              </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://leezekun.github.io/" target="_blank">Zekun Li</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=u1CNjgwAAAAJ&hl=en&oi=ao" target="_blank">Baolin Peng</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=TS1RoxAAAAAJ&hl=en" target="_blank">Pengcheng He</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://scholar.google.com/citations?hl=en&user=rs1M7CAAAAAJ" target="_blank">Michel Galley</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=CQ1cqKkAAAAJ&hl=en&oi=ao" target="_blank">Jianfeng Gao</a><sup>2†</sup>,</span>
                        <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=XZV2eogAAAAJ&hl=en" target="_blank">Xifeng Yan</a><sup>1†</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of California, Santa Barbara<br>Microsoft</span><br>37th Conference on Neural Information Processing Systems (NeurIPS 2023)</span>
                    <span class="eql-cntrb"><small><br>{zekunli, xyan}@cs.ucsb.edu</small></span>
                    <span class="eql-cntrb"><small><br>{bapeng,penhe,mgalley,jfgao}@microsoft.com</small></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Part of the work was done when Zekun Li was interning at Microsoft Research.</small></span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>†</sup>Co-advise on this work.</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2302.11520.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Leezekun/Directional-Stimulus-Prompting" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2302.11520" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column is-four-fifths">
                  <div class="item">
                      <!-- Your image here -->
                      <img src="static/images/example_v2.jpg" alt="Comparison of our Directional Stimulus Prompting and the standard prompting method using LLMs such as ChatGPT for the summarization task." />
                      <h2 class="subtitle;columns is-centered">
                          Figure 1: Comparison of our Directional Stimulus Prompting and the standard prompting method using LLMs such as ChatGPT for the summarization task. 
                          DSP introduces an auxiliary <b>directional stimulus prompts, i.e., hints</b> (highlighted in orange), which are keywords in this case, to provide instance-specific guidance to LLMs in generating summaries (highlighted in blue) that better align with the desired reference summary with higher ROUGE scores or other measures like human preferences.
                      </h2>
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. 
            Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an <b>auxiliary directional stimulus prompt</b> for each input instance. 
            These directional stimulus prompts act as <b>nuanced, instance-specific hints and clues</b> to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. 
            Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. 
            The policy model can be optimized through 1) <b>supervised fine-tuning</b> using labeled data and 2) <b>reinforcement learning</b> from offline or online rewards based on the LLM's output. 
            We assess our method across <b>summarization</b>, <b>dialogue response generation</b>, and <b>chain-of-thought reasoning</b> tasks. 
            Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. 
            Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. 
            Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts, showcasing its effectiveness as an <b>automatic prompt engineering/optimization</b> approach.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered is-fifths-fifths">
                <h2 style:"text-align: center;" class="title is-3">Approach</h2>
                  <div class="item">
                      <!-- Your image here -->
                      <img src="static/images/overview_page-0001.jpg" alt="overview_page-0001.jpg" />
                      <h2 class="subtitle;columns is-centered">
                          Figure 2: Overview of our proposed framework DSP, where <b>we learn a small tunable policy model
                          to generate the directional stimulus prompts (keywords in this case) that provide input-specific guidance for
                          the LLM toward the desired target</b>. <b>The policy model can be trained with SFT and/or RL, where the
                          reward is defined as the downstream task performance measure</b>, such as the ROUGE score for the
                          summarization task, or other alignment measures like human preferences.
                      </h2>
                  </div>
              </div>
          </div>
          <div class="column is-fifths-fifths">
            <p style="margin-left: 2em;margin-right: 2em;">We utilize a relatively small and tunable LM (e.g., T5), as the policy model to generate the directional
              stimulus prompt for each input query. This approach enables us to sidestep the direct optimization
              of black-box LLMs by optimizing the small tunable policy model instead. We train the policy
              model through supervised fine-tuning (SFT) using a few collected labeled data. After supervised
              fine-tuning, we further optimize the policy model to explore better directional stimulus prompts
              with reinforcement learning (RL). During RL training, we aim to maximize the reward defined as
              downstream performance measures or any other measures of the LLM's output conditioned on the
              stimulus generated by the policy model.<br /></p> 
            </p>
        </div>
      </div>
  </div>
</section>
<!--approach end-->


<!-- experiment1 carousel -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered is-fifths-fifths">
                <h2 style:"text-align: center;" class="title is-3">Experiments</h2><br />
                  <div class="item">
                      <h2 class="subtitle;columns is-centered">
                        <p><b>Our framework can be flexibly applied to into various tasks and LMs by selecting the appropriate directional stimulus prompts (i.e., hints) and reward functions.</b></p>
                        In this work, we spotlight its utility by focusing on three key applications: <b>summarization</b>, <b>dialogue response generation</b>, and <b>chain-of-thought reasoning</b>, which encompasses <b>automatic prompt engineering/optimization</b>.
                        We experiment with the black-box LLMs including <b>OpenAI's ChatGPT (gpt-3.5-turbo), Codex (code-davinci-002), and InstructGPT (text-davinci-002)</b>, and use pre-trained <b>T5 or Flan-T5</b> to initialize the policy model.
                        <br /><br/>
                      </h2>
                      <h2 class="subtitle;"><b>1. Summarization</b><br/><br/></h2>
                      <!-- Your image here -->
                      <img src="static/images/cnndm_exp_chatgpt2_page-0001.jpg" alt="cnndm_exp_chatgpt2_page-0001.jpg" />
                      <h2 class="subtitle;columns is-centered">
                        Figure 3: Performance comparison of ChatGPT with standard prompting and DSP trained with SFT
                        and SFT+RL, using varying numbers of training samples from the CNN/Daily Mail dataset.
                      </h2>
                  </div>
              </div>
          </div>
          <div class="column is-fifths-fifths">
            <p style="margin-left: 2em;margin-right: 2em;"><br />All the evaluation scores improve with our proposed DSP compared with standard prompting. 
              Supervised fine-tuning (SFT) improves the benchmark performance, while additional <b>RL</b> results in further performance improvement, as it effectively encourages the policy model to explore better directional stimulus that maximizes the reward. 
              Despite using a small collection of only 1,000 to 4,000 samples to keep API usage costs low, our DSP approach still consistently enhances ChatGPT’s ROUGE, BLEU, and Meteor scores,  even though ChatGPT has already achieved considerable performance.</p>
              <br/>
            </p>
        </div>
        <div class="columns is-centered">
          <div class="column has-text-centered is-fifths-fifths">
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/training_curve-1000.jpg" 
                    width="400" 
                    height="300" 
                    alt="training_curve-1000.jpg" />
                    <h2 class="subtitle;columns is-centered">
                      Figure 4: Training curve on 1000 samples from the CNN/Daily Mail dataset.
                    </h2>
                </div>
            </div>
        </div>
        <div class="column is-fifths-fifths">
          <p style="margin-left: 2em;margin-right: 2em;"><br />Figure 4 presents the change of training rewards and ROUGE-1 score on the validation set during the training
            process on 1,000 samples. We can see that the performance is closely related to the training rewards, and the training is relatively stable using the NLPO algorithm.<br/><br/>
            To gain a better understanding of generated summaries guided by keywords, we employed <b>GPT-4</b> to evaluate 
            the summaries. <br/>
            </p>
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column has-text-centered is-fifths-fifths">
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/gpt4_eval_pie_chart.jpg" 
                    width="400" 
                    height="300" 
                    alt="training_curve-1000.jpg" />
                    <h2 class="subtitle;columns is-centered">
                      Figure 5: GPT-4 evaluation on comparing the summaries generated with our approach DSP, 
                      i.e., with the guidance of our generated keywords, and the original standard prompting, 
                      i.e., without keyword hint guidance.
                    </h2>
                </div>
            </div>
        </div>
        <div class="column is-fifths-fifths">
          <p style="margin-left: 2em;margin-right: 2em;"><br />We found that GPT-4 can 
            produce reasonable and detailed explanations of their assessment. From our test set of 500 samples: DSP-generated 
            summaries were favored 255 times (51.0%), summaries generated with original standard prompting were favored 222 
            times (44.4%), while a tie was observed in 23 cases (4.6%).
            </p>
          </p>
        </div>
      </div>
  </div>
</section>


<!-- experiment2 carousel -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered is-fifths-fifths">
              <div class="item">
                <h2 class="subtitle;"><b>2. Dialogue Response Generation</b><br/><br/></h2>
                <p>Current LLM-based chatbots such as ChatGPT and Claude are typically targeted at open-domain conversations while struggling with <b>task-oriented (goal-oriented) dialogues</b> where they need to assist users in completing specific goals or tasks, such as making reservations or ordering food. 
                  Unlike open-domain conversations, task-oriented dialogues often require the chatbot to follow task-specific business logic and respond based on reliable information from API calls or database queries.
                  To address this limitation, we train the policy model to generate <b>dialogue acts</b>, which are used to guide the LLMs in generate reliable system responses that assist users in completing tasks and goals.</p>
                <!-- Your image here -->
                <img src="static/images/table1.png" alt="table1.png" />
                <h2 class="subtitle;columns is-centered">
                  Table 1: Response generation performance of different methods on the MultiWOZ 2.0&2.1 datasets, 
                  where Succ. and Comb. denote the Success and Combined Score metrics, respectively.
                </h2>
              </div>
            </div>
          </div>      
          <div class="column is-fifths-fifths">
            <p style="margin-left: 2em;margin-right: 2em;"><br /> 
              From Table 1 we can see that our approach DSP significantly improves the success and inform rates of Codex and ChatGPT, even with only 80 dialogues, 
              surpassing several fully trained TOD models, particularly in terms of Success and Inform rates.</p>
              <br/>
            </p>
          </div>
      </div>
  </div>
</section>

<!-- experiment3 carousel -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered is-fifths-fifths">
              <div class="item">
                <h2 class="subtitle;"><b>3. Chain-of-Thought reasoning</b><br/><br/></h2>
                <!-- Your image here -->
                <img src="static/images/table2.jpg" alt="table2.jpg" />
                <h2 class="subtitle;columns is-centered">
                  Table 2: Zero-shot chain of thoughts performance of InstructGPT (text-davinci-002) with different prompts. 
                  *Our approach trains a policy model to generate instance-specific prompt triggers, which are compared to 
                  the task-specific prompts in [26, 79].

                </h2>
              </div>
            </div>
          </div>
          <div class="column is-fifths-fifths">
            <p style="margin-left: 2em;margin-right: 2em;"><br /> As can be seen in Table 2, InstructGPT’s performance
              varies significantly when using different task-specific prompts. Compared to the 14 task-specific
              human-designed prompts, DSP enhances the performance with instance-specific prompts. It also
              outperforms the prompt discovered by the APE approach. Solely relying on supervised fine-tuning
              of the policy model with the dataset comprising the 14 human-designed prompts doesn’t lead to
              its peak performance. After fine-tuning with RL, the policy model is encouraged to explore better
              instance-specific trigger prompts, further improving performance.</p>
              <br/>
            </p>
          </div>
      </div>
  </div>
</section>

<!-- conclusions -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusions and Future Work</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we introduce <b>Directional Stimulus Prompting (DSP)</b>, a new prompting framework to provide black-box LLMs with fine-grained and instance-specific guidance toward the desired outputs.
We use a tunable policy model to generate the directional stimulus to provide such guidance and convert the optimization of black-box LLMs to that of the policy model.
Experimental results demonstrate the effectiveness of our approach in controlling and guiding black-box LLMs via automatic prompt engineering and optimization. 
Furthermore, the generated stimulus provides valuable insights and interpretations of LLMs' behaviors.
In this work, we use heuristically selected or annotated pseudo-stimulus data for supervised fine-tuning of the policy model. 
For future work, we hope to explore the possibility of using a “machine language” between the policy model and the LLMs that might not be intuitively preferred by humans but can better convey guidance information, as well as other forms of directional stimulus beyond text.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End conclusions -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{li2023guiding,
        title={Guiding Large Language Models via Directional Stimulus Prompting}, 
        author={Zekun Li and Baolin Peng and Pengcheng He and Michel Galley and Jianfeng Gao and Xifeng Yan},
        year={2023},
        eprint={2302.11520},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
